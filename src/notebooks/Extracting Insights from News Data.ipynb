{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Insights from Text Using spaCy's Pipeline\n",
    "\n",
    "## NLP Day, November 7th 2019\n",
    "\n",
    "http://nlpday.ml/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprerations\n",
    "\n",
    "First, we need to import all neccesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy’s models can be installed as Python packages. This means that they’re a component of your application, just like any other module. They’re versioned and can be defined as a dependency in your requirements.txt. Models can be installed from a download URL or a local directory, manually or via pip. Their data can be located anywhere on your file system.\n",
    "\n",
    "https://spacy.io/usage/models\n",
    "\n",
    "For this exercise, we will use the `en_core_web_sm` model, and it can be installed with the command:\n",
    "`python -m spacy download en_core_web_sm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the default fields we want to print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELDS = ['text', 'lemma', 'tag', 'pos', 'head', 'dep', 'ent_iob', 'ent_type', 'idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some helper methods for printing the parsed sentence after spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_data(token):\n",
    "    # https://spacy.io/api/token\n",
    "    return dict(\n",
    "        idx=token.idx,\n",
    "        i=token.i,\n",
    "        text=token.text,\n",
    "        norm=token.norm_,\n",
    "        head=token.head.i,\n",
    "        dep=token.dep_,\n",
    "        lemma=token.lemma_,\n",
    "        pos=token.pos_,\n",
    "        tag=token.tag_,\n",
    "        ent_type=token.ent_type_,\n",
    "        ent_iob=token.ent_iob_,\n",
    "    )\n",
    "\n",
    "def json_tree(parsed_sentence):\n",
    "    return [token_data(t) for t in parsed_sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to parse a sentence with spaCy, one need to call the model, with a given string. Simple as that!\n",
    "The return object's structures is as follows: https://spacy.io/api/doc#attributes. The Doc object contains a list of tokens, each one of the structure: https://spacy.io/api/token#attributes. For each token, its data depends on the components in the pipeline. In our case, we are using the default pipeline, which is:\n",
    "\n",
    "1) Tokenizer - https://spacy.io/api/tokenizer\n",
    "\n",
    "2) Tagger - https://spacy.io/api/tagger\n",
    "\n",
    "3) Dependency Parser - https://spacy.io/api/dependencyparser\n",
    "\n",
    "4) Named Entity Recognition - https://spacy.io/api/entityrecognizer\n",
    "\n",
    "5) Text Categorizer - https://spacy.io/api/textcategorizer\n",
    "\n",
    "\n",
    "For more details:\n",
    "https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a simple sentence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentence = nlp('Apple Inc is a great company!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'dep': 'compound',\n",
      "  'ent_iob': 'B',\n",
      "  'ent_type': 'ORG',\n",
      "  'head': 1,\n",
      "  'i': 0,\n",
      "  'idx': 0,\n",
      "  'lemma': 'Apple',\n",
      "  'norm': 'apple',\n",
      "  'pos': 'PROPN',\n",
      "  'tag': 'NNP',\n",
      "  'text': 'Apple'},\n",
      " {'dep': 'nsubj',\n",
      "  'ent_iob': 'I',\n",
      "  'ent_type': 'ORG',\n",
      "  'head': 2,\n",
      "  'i': 1,\n",
      "  'idx': 6,\n",
      "  'lemma': 'Inc',\n",
      "  'norm': 'inc',\n",
      "  'pos': 'PROPN',\n",
      "  'tag': 'NNP',\n",
      "  'text': 'Inc'},\n",
      " {'dep': 'ROOT',\n",
      "  'ent_iob': 'O',\n",
      "  'ent_type': '',\n",
      "  'head': 2,\n",
      "  'i': 2,\n",
      "  'idx': 10,\n",
      "  'lemma': 'be',\n",
      "  'norm': 'is',\n",
      "  'pos': 'VERB',\n",
      "  'tag': 'VBZ',\n",
      "  'text': 'is'},\n",
      " {'dep': 'det',\n",
      "  'ent_iob': 'O',\n",
      "  'ent_type': '',\n",
      "  'head': 5,\n",
      "  'i': 3,\n",
      "  'idx': 13,\n",
      "  'lemma': 'a',\n",
      "  'norm': 'a',\n",
      "  'pos': 'DET',\n",
      "  'tag': 'DT',\n",
      "  'text': 'a'},\n",
      " {'dep': 'amod',\n",
      "  'ent_iob': 'O',\n",
      "  'ent_type': '',\n",
      "  'head': 5,\n",
      "  'i': 4,\n",
      "  'idx': 15,\n",
      "  'lemma': 'great',\n",
      "  'norm': 'great',\n",
      "  'pos': 'ADJ',\n",
      "  'tag': 'JJ',\n",
      "  'text': 'great'},\n",
      " {'dep': 'attr',\n",
      "  'ent_iob': 'O',\n",
      "  'ent_type': '',\n",
      "  'head': 2,\n",
      "  'i': 5,\n",
      "  'idx': 21,\n",
      "  'lemma': 'company',\n",
      "  'norm': 'company',\n",
      "  'pos': 'NOUN',\n",
      "  'tag': 'NN',\n",
      "  'text': 'company'},\n",
      " {'dep': 'punct',\n",
      "  'ent_iob': 'O',\n",
      "  'ent_type': '',\n",
      "  'head': 2,\n",
      "  'i': 6,\n",
      "  'idx': 28,\n",
      "  'lemma': '!',\n",
      "  'norm': '!',\n",
      "  'pos': 'PUNCT',\n",
      "  'tag': '.',\n",
      "  'text': '!'}]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(json_tree(parsed_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "Let's load the data we have and explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.read_csv('../../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>company</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>British Telecom is planning to launch Qumu as ...</td>\n",
       "      <td>British Telecom</td>\n",
       "      <td>Qumu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Also in this quarter, the first WattUp enabled...</td>\n",
       "      <td>Delight</td>\n",
       "      <td>WattUp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In September, SEAT will expand its SUV range w...</td>\n",
       "      <td>SEAT</td>\n",
       "      <td>Tarraco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The new Jetta being launched now by SEAT and t...</td>\n",
       "      <td>SEAT</td>\n",
       "      <td>Jetta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The employee from Fiat contributed personally ...</td>\n",
       "      <td>Fiat</td>\n",
       "      <td>Fiorino</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence          company  product\n",
       "0  British Telecom is planning to launch Qumu as ...  British Telecom     Qumu\n",
       "1  Also in this quarter, the first WattUp enabled...          Delight   WattUp\n",
       "2  In September, SEAT will expand its SUV range w...             SEAT  Tarraco\n",
       "3  The new Jetta being launched now by SEAT and t...             SEAT    Jetta\n",
       "4  The employee from Fiat contributed personally ...             Fiat  Fiorino"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each line contains:\n",
    "\n",
    "1) A sentence that talks about a company the released (or going to release) a new product\n",
    "\n",
    "2) The company name\n",
    "\n",
    "3) The product name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 100\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(products_df)\n",
    "print(f'Number of rows: {num_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have very few rows in our data set** :(\n",
    "\n",
    "Unfortunately, a statistical model or a neural network may not work here (but we enourage you to try!), so we may use a rule-base approach utilizing spaCy's pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that not all rows contains company and product in them, so our rule-based model should be agnostic for that cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows without a company or a product\n",
      "sentence     0\n",
      "company     11\n",
      "product     10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "num_rows_without_company_or_product = products_df.isnull().sum()\n",
    "print(f'Number of rows without a company or a product\\n{num_rows_without_company_or_product}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = products_df[pd.isnull(products_df).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>company</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Let's say, we'll announce the product when we ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>We've got the product.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>As I mentioned, we launched some new products.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>I knowthey just launched the Dealership Finance.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dealership Finance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>There is getting new product launched, somethi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence company  \\\n",
       "66  Let's say, we'll announce the product when we ...     NaN   \n",
       "67                             We've got the product.     NaN   \n",
       "68     As I mentioned, we launched some new products.     NaN   \n",
       "69   I knowthey just launched the Dealership Finance.     NaN   \n",
       "70  There is getting new product launched, somethi...     NaN   \n",
       "\n",
       "               product  \n",
       "66                 NaN  \n",
       "67                 NaN  \n",
       "68                 NaN  \n",
       "69  Dealership Finance  \n",
       "70                 NaN  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-Based Model\n",
    "\n",
    "Now, let's begin with the most fun part - our model!\n",
    "\n",
    "The goal is to write NLP rules, based on spaCy's data in order to extract a company name and their new released product from a given sentence (or None if not available in the sentence) based on the examples we have in the training data.\n",
    "\n",
    "We will start with simple rules and expend it to more complicated ones in order to push our accuracy higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We will use the **accuracy** metric as evaluation metric. For each row, the point will be split to 0.5 of the scor for the company name and 0.5 for the product name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(companies_true, products_true, companies_predicted, products_predicted):\n",
    "    correct_companies = [1 if company_true == company_predicted else 0\n",
    "                            for company_true, company_predicted in zip(companies_true, companies_predicted)]\n",
    "    correct_products = [1 if product_true == product_predicted else 0\n",
    "                        for product_true, product_predicted in zip(products_true, products_predicted)]\n",
    "    \n",
    "    return float((sum(correct_companies) + sum(correct_products))) / (len(correct_companies) + len(correct_products)) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our `accuracy_score` method above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 75.0\n"
     ]
    }
   ],
   "source": [
    "test_score = accuracy_score(['Apple', 'Amazon'], ['iPhone', 'AWS'], ['Google', 'Amazon'], ['iPhone', 'AWS'])\n",
    "assert(test_score == 75)\n",
    "print(f'Test score: {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Model: Using POS only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Write rules to extract company name and product using the `tag_` and `pos_` fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2st Model: Using NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** In addition to eariler, now change or add new rules using the `ent_type_` and `ent_iob_` fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3st Model: Using Dependency Tree arcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** In addition to eariler, now change or add new rules using the `dep_` and `head` fields"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
